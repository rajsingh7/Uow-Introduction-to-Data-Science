---
title: "Untitled"
author: "Ryan Cheung"
date: "Thursday, August 14, 2014"
output: html_document
---

### Install and Load Packages
```{r}
#install.packages("caret")
#install.packages("rpart")
#install.packages("tree")
#install.packages("randomForest")
#install.packages("e1071")
#install.packages("ggplot2")
#install.packages("caTools")
library(caret)
library(rpart)
library(tree)
library(randomForest)
library(e1071)
library(ggplot2)
library(caTools)
```

### Read in Data
```{r}
data <- read.csv("seaflow_21min.csv")
```

### Look at the Summary
```{r}
summary(data)
```

## Step 1: Read and summarize the data

### Question 1   
How many particles labeled "synecho" are in the file provided?
```{r}
sum(data$pop == 'synecho')
summary(data$pop)
table(data$pop)
```

### Question 2
What is the 3rd Quantile of the field fsc_small? 
```{r}
summary(data$fsc_small)
sort(data$fsc_small)[nrow(data)*0.75]
```

## Step 2: Split the data into test and training sets

```{r}
set.seed(123)
split <- sample.split(data$pop, SplitRatio = 1/2)
train <- subset(data, split == T)
test <- subset(data, split == F)
```

### Question 3
What is the mean of the variable "time" for your training set?
```{r}
mean(train$time)
```

## Step 3: Plot the data
```{r}
ggplot(train, aes(x = pe, y = chl_small, color = pop)) + geom_point()
```

### Question 4
In the plot of pe vs. chl_small, the particles labeled ultra should appear to be somewhat "mixed" with two other populations of particles. Which two populations?

 pico nano



## Step 4: Train a decision tree.

```{r}
model <- rpart(pop~fsc_small + fsc_perp + fsc_big + pe + chl_big + chl_small, method = 'class', data = train)
```


```{r}
print(model)
plot(model)
text(model)
summary(train$pop)
```

### Question 5
Use print(model) to inspect your tree. Which populations, if any, is your tree incapable of recognizing? (Which populations do not appear on any branch?) (It's possible, but very unlikely, that an incorrect answer to this question is the result of improbable sampling.)

crypto

### Question 6
Most trees will include a node near the root that applies a rule to the pe field, where particles with a value less than some threshold will descend down one branch, and particles with a value greater than some threshold will descend down a different branch. If you look at the plot you created previously, you can verify that the threshold used in the tree is evident visually. What is the value of the threshold on the pe field learned in your model?

5006.5

### Question 7
Based on your decision tree, which variables appear to be most important in predicting the class population?

chl_small pe

## Step 5: Evaluate the decision tree on the test data.
```{r}
pred <- predict(model, newdata = test, type = "class")
sum(pred == test$pop)/nrow(test)
table <- table(prediction = pred, truth = test$pop)
```

### Question 8
How accurate was your decision tree on the test data? Enter a number between 0 and 1.

## Step 6: Build and evaluate a random forest.
```{r}
modelRF <- randomForest(pop~fsc_small + fsc_perp + fsc_big + pe + chl_big + chl_small, method = 'class', data = train)
```

### Question 9
What was the accuracy of your random forest model on the test data? Enter a number between 0 and 1.

```{r}
predRF <- predict(modelRF, newdata = test, type = "class")
sum(predRF == test$pop)/nrow(test)
tableRF <- table(prediction = predRF, truth = test$pop)
```

### Question 10
After calling importance(model), you should be able to determine which variables appear to be most important in terms of the gini impurity measure. Which ones are they?

```{r}
importance(modelRF)
```

pe chl_small

## Step 7: Train a support vector machine model and compare results.
```{r}
modelSVM <- svm(pop~fsc_small + fsc_perp + fsc_big + pe + chl_big + chl_small, method = 'class', data = train)
```

### Question 11
What was the accuracy of your support vector machine model on the test data? Enter a number between 0 and 1.

```{r}
predSVM <- predict(modelSVM, newdata = test, type = "class")
sum(predSVM == test$pop)/nrow(test)
tableSVM <- table(prediction = predSVM, truth = test$pop)
```

## Step 8: Construct confusion matrices
### Question 12
Construct a confusion matrix for each of the three methods using the table function. What appears to be the most common error the models make?

ultra is mistaken for pico
```{r}
tableSVM
tableRF
table
```

## Step 9: Sanity check the data
### Question 13
The variables in the dataset were assumed to be continuous, but one of them takes on only a few discrete values, suggesting a problem. Which variable exhibits this problem?
```{r}
length(unique(data$fsc_big))
length(unique(data$fsc_small))
length(unique(data$fsc_perp))
length(unique(data$pe))
length(unique(data$chl_small))
length(unique(data$chl_big))
```

fsc_big

### Question 14
After removing data associated with file_id 208, what was the effect on the accuracy of your svm model? Enter a positive or negative number representing the net change in accuracy, where a positive number represents an improvement in accuracy and a negative number represents a decrease in accuracy.

```{r}
data208rm <- subset(data, data$file_id != 208) 
set.seed(123)
split208rm <- sample.split(data208rm$pop, SplitRatio = 1/2)
train208rm <- subset(data208rm, split208rm == T)
test208rm <- subset(data208rm, split208rm == F)
modelSVM208rm <- svm(pop~fsc_small + fsc_perp + fsc_big + pe + chl_big + chl_small, method = 'class', data = train208rm)
predSVM208rm <- predict(modelSVM208rm, newdata = test208rm, type = "class")
sum(predSVM208rm == test208rm$pop)/nrow(test208rm)-sum(predSVM == test$pop)/nrow(test)
```

